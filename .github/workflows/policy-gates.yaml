# SPDX-License-Identifier: Apache-2.0
# Copyright 2025 Provability-Fabric Contributors

name: Policy Gates

on:
  push:
    branches: [main]
    paths:
      - "proofs/**"
      - "core/lean-libs/**"
      - "runtime/sidecar-watcher/**"
      - "artifact/**"
      - ".github/workflows/policy-gates.yaml"
  pull_request:
    branches: [main]
    paths:
      - "proofs/**"
      - "core/lean-libs/**"
      - "runtime/sidecar-watcher/**"
      - "artifact/**"
      - ".github/workflows/policy-gates.yaml"

jobs:
  # Job 1: Build proofs and export DFAs
  build-proofs-dfa:
    runs-on: ubuntu-latest
    timeout-minutes: 45
    strategy:
      matrix:
        lean-version: [leanprover/lean4:nightly-2024-12-01]

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          submodules: true

      - name: Set up Lean
        run: |
          # Install Lean 4
          curl -L https://github.com/leanprover/lean4/releases/download/v4.7.0/lean-4.7.0-linux.tar.gz | tar xz
          echo "$PWD/lean-4.7.0-linux/bin" >> $GITHUB_PATH
          echo "LEAN_PATH=$PWD/lean-4.7.0-linux" >> $GITHUB_ENV

      - name: Build Lean proofs
        run: |
          cd proofs
          lake build
          echo "✅ Lean proofs built successfully"

      - name: Export DFA
        run: |
          cd proofs
          lake exe export-dfa
          echo "✅ DFA exported successfully"

      - name: Export Labeler
        run: |
          cd proofs
          lake exe export-labeler
          echo "✅ Labeler exported successfully"

      - name: Verify DFA artifacts
        run: |
          if [ -f "artifact/dfa.json" ]; then
            echo "✅ DFA artifact generated"
            echo "DFA size: $(wc -l < artifact/dfa.json) lines"
          else
            echo "❌ DFA artifact not found"
            exit 1
          fi

      - name: Upload DFA artifacts
        uses: actions/upload-artifact@v4
        with:
          name: dfa-artifacts
          path: artifact/dfa.json
          retention-days: 7

  # Job 2: Run replay suites
  replay-suites:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    needs: build-proofs-dfa
    strategy:
      matrix:
        suite: [accept, deny]

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          submodules: true

      - name: Download DFA artifacts
        uses: actions/download-artifact@v4
        with:
          name: dfa-artifacts
          path: artifact/

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-cov

      - name: Run replay suite - ${{ matrix.suite }}
        run: |
          cd tests/replay
          if [ "${{ matrix.suite }}" = "accept" ]; then
            echo "Running good trace suite..."
            python run_replay.py --suite good --dfa-path ../../artifact/dfa.json
          else
            echo "Running bad trace suite..."
            python run_replay.py --suite bad --dfa-path ../../artifact/dfa.json
          fi

      - name: Verify replay results
        run: |
          if [ "${{ matrix.suite }}" = "accept" ]; then
            echo "✅ Good traces should all pass"
            # Add verification logic here
          else
            echo "✅ Bad traces should all be denied"
            # Add verification logic here
          fi

  # Job 3: Property tests
  property-tests:
    runs-on: ubuntu-latest
    timeout-minutes: 40
    needs: build-proofs-dfa

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          submodules: true

      - name: Download DFA artifacts
        uses: actions/download-artifact@v4
        with:
          name: dfa-artifacts
          path: artifact/

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest hypothesis fuzzingbook

      - name: Run adversarial egress chunking tests
        run: |
          cd tests/property
          python test_egress_chunking.py --dfa-path ../../artifact/dfa.json
          echo "✅ Egress chunking tests passed"

      - name: Run polyglot JSON labeler tests
        run: |
          cd tests/property
          python test_polyglot_json.py --dfa-path ../../artifact/dfa.json
          echo "✅ Polyglot JSON tests passed"

      - name: Run ABAC permutations tests
        run: |
          cd tests/property
          python test_abac_permutations.py --dfa-path ../../artifact/dfa.json
          echo "✅ ABAC permutations tests passed"

  # Job 4: Coverage report
  coverage-report:
    runs-on: ubuntu-latest
    timeout-minutes: 20
    needs: [build-proofs-dfa, replay-suites, property-tests]

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          submodules: true

      - name: Download DFA artifacts
        uses: actions/download-artifact@v4
        with:
          name: dfa-artifacts
          path: artifact/

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install coverage

      - name: Generate coverage report
        run: |
          cd tools/coverage
          python generate_coverage.py \
            --dfa-path ../../artifact/dfa.json \
            --proofs-path ../../proofs \
            --tests-path ../../tests \
            --output-path coverage_report.json

      - name: Parse coverage metrics
        id: coverage
        run: |
          cd tools/coverage
          python parse_coverage.py coverage_report.json

          # Extract metrics for output
          TOOLS_COVERED=$(python -c "import json; data=json.load(open('coverage_report.json')); print(data.get('tools_coverage_percent', 0))")
          EVENTS_COVERED=$(python -c "import json; data=json.load(open('coverage_report.json')); print(data.get('events_coverage_percent', 0))")
          EGRESS_LABELED=$(python -c "import json; data=json.load(open('coverage_report.json')); print(data.get('egress_labeled_percent', 0))")
          REPLAY_DETERMINISM=$(python -c "import json; data=json.load(open('coverage_report.json')); print(data.get('replay_determinism_percent', 0))")

          echo "tools_covered=$TOOLS_COVERED" >> $GITHUB_OUTPUT
          echo "events_covered=$EVENTS_COVERED" >> $GITHUB_OUTPUT
          echo "egress_labeled=$EGRESS_LABELED" >> $GITHUB_OUTPUT
          echo "replay_determinism=$REPLAY_DETERMINISM" >> $GITHUB_OUTPUT

      - name: Upload coverage report
        uses: actions/upload-artifact@v4
        with:
          name: coverage-report
          path: tools/coverage/coverage_report.json
          retention-days: 30

      - name: Comment coverage on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const { tools_covered, events_covered, egress_labeled, replay_determinism } = context.job.steps.coverage.outputs;

            const comment = `## 📊 Policy Coverage Report

            **Tools Coverage:** ${tools_covered}%
            **Events Coverage:** ${events_covered}%
            **Egress Labeled:** ${egress_labeled}%
            **Replay Determinism:** ${replay_determinism}%

            All policy gates passed successfully! ✅`;

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });

  # Job 5: Policy validation
  policy-validation:
    runs-on: ubuntu-latest
    timeout-minutes: 25
    needs: build-proofs-dfa

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          submodules: true

      - name: Download DFA artifacts
        uses: actions/download-artifact@v4
        with:
          name: dfa-artifacts
          path: artifact/

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install jsonschema

      - name: Validate DFA schema
        run: |
          cd tools/validation
          python validate_dfa_schema.py \
            --dfa-path ../../artifact/dfa.json \
            --schema-path ../../config/schemas/dfa-schema.json
          echo "✅ DFA schema validation passed"

      - name: Validate policy consistency
        run: |
          cd tools/validation
          python validate_policy_consistency.py \
            --dfa-path ../../artifact/dfa.json \
            --proofs-path ../../proofs
          echo "✅ Policy consistency validation passed"

      - name: Check for policy conflicts
        run: |
          cd tools/validation
          python check_policy_conflicts.py \
            --dfa-path ../../artifact/dfa.json
          echo "✅ No policy conflicts detected"

  # Job 6: Performance benchmarks
  performance-benchmarks:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    needs: build-proofs-dfa

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          submodules: true

      - name: Download DFA artifacts
        uses: actions/download-artifact@v4
        with:
          name: dfa-artifacts
          path: artifact/

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest-benchmark

      - name: Run performance benchmarks
        run: |
          cd bench
          python performance_benchmarks.py \
            --dfa-path ../artifact/dfa.json \
            --output-path benchmark_results.json

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: bench/benchmark_results.json
          retention-days: 30

      - name: Check performance regressions
        run: |
          cd bench
          python check_performance_regressions.py \
            --current-results benchmark_results.json \
            --baseline-path ../artifact/performance_baseline.json
          echo "✅ Performance regression check passed"

# Note: Global failure handling would be implemented in a separate workflow
# or through GitHub's built-in notification system
