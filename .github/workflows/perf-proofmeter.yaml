name: ProofMeter Performance

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]
  schedule:
    # Run weekly on Mondays at 3 AM UTC
    - cron: "0 3 * * 1"

jobs:
  perf-smoke:
    name: Performance Smoke Test (PR)
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'
    timeout-minutes: 10

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Go
        uses: actions/setup-go@v4
        with:
          go-version: "1.23"

      - name: Build benchmark tool
        run: |
          cd tests/perf
          go build -o proofmeter_bench proofmeter_bench.go

      - name: Start ProofMeter service
        run: |
          # Start ProofMeter service in background
          # This would be replaced with actual service startup
          echo "Starting ProofMeter service..."
          # Example: docker run -d -p 8080:8080 proofmeter:latest
          # For now, we'll simulate the service
          sleep 5

      - name: Run smoke test
        run: |
          cd tests/perf
          ./proofmeter_bench \
            --rps 2000 \
            --duration 60s \
            --concurrency 20 \
            --endpoint http://localhost:8080/proof \
            --output smoke_results.json \
            --seed 12345

      - name: Upload results
        uses: actions/upload-artifact@v4
        with:
          name: smoke-results
          path: tests/perf/smoke_results.json

      - name: Check gates
        run: |
          cd tests/perf
          if [ -f smoke_results.json ]; then
            echo "‚úÖ Smoke test completed"
            echo "Results:"
            cat smoke_results.json | jq '.summary'
          else
            echo "‚ùå Smoke test failed - no results file"
            exit 1
          fi

  perf-full:
    name: Performance Full Test (Weekly)
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || github.event_name == 'push'
    timeout-minutes: 30

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Go
        uses: actions/setup-go@v4
        with:
          go-version: "1.23"

      - name: Build benchmark tool
        run: |
          cd tests/perf
          go build -o proofmeter_bench proofmeter_bench.go

      - name: Start ProofMeter service
        run: |
          # Start ProofMeter service in background
          echo "Starting ProofMeter service..."
          # Example: docker run -d -p 8080:8080 proofmeter:latest
          sleep 10

      - name: Run full benchmark
        run: |
          cd tests/perf
          ./proofmeter_bench \
            --rps 5000 \
            --duration 10m \
            --concurrency 50 \
            --endpoint http://localhost:8080/proof \
            --output full_results.json \
            --seed 12345

      - name: Upload results
        uses: actions/upload-artifact@v4
        with:
          name: full-results
          path: tests/perf/full_results.json

      - name: Generate performance report
        run: |
          cd tests/perf
          if [ -f full_results.json ]; then
            echo "üìä Performance Report"
            echo "===================="
            
            # Extract key metrics
            P95_LATENCY=$(cat full_results.json | jq -r '.summary.p95_latency')
            SUCCESS_RATE=$(cat full_results.json | jq -r '.summary.success_rate')
            ACTUAL_RPS=$(cat full_results.json | jq -r '.summary.actual_rps')
            TOTAL_REQUESTS=$(cat full_results.json | jq -r '.summary.total_requests')
            
            echo "P95 Latency: ${P95_LATENCY}ms"
            echo "Success Rate: ${SUCCESS_RATE}%"
            echo "Actual RPS: ${ACTUAL_RPS}"
            echo "Total Requests: ${TOTAL_REQUESTS}"
            
            # Check gates
            LATENCY_MS=$(echo $P95_LATENCY | sed 's/ms//')
            if (( $(echo "$LATENCY_MS > 20" | bc -l) )); then
              echo "‚ùå P95 latency ${LATENCY_MS}ms exceeds 20ms gate"
              exit 1
            fi
            
            if (( $(echo "$SUCCESS_RATE < 99.5" | bc -l) )); then
              echo "‚ùå Success rate ${SUCCESS_RATE}% below 99.5% gate"
              exit 1
            fi
            
            echo "‚úÖ All performance gates passed!"
          else
            echo "‚ùå Full benchmark failed - no results file"
            exit 1
          fi

      - name: Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const results = JSON.parse(fs.readFileSync('tests/perf/full_results.json', 'utf8'));

            const comment = `## ProofMeter Performance Results

            **P95 Latency:** ${results.summary.p95_latency}ms (target: ‚â§20ms)
            **Success Rate:** ${results.summary.success_rate.toFixed(2)}% (target: ‚â•99.5%)
            **Actual RPS:** ${results.summary.actual_rps.toFixed(2)} (target: 5000)
            **Total Requests:** ${results.summary.total_requests}

            ### Latency Breakdown:
            - P50: ${results.summary.p50_latency}ms
            - P95: ${results.summary.p95_latency}ms
            - P99: ${results.summary.p99_latency}ms
            - Min: ${results.summary.min_latency}ms
            - Max: ${results.summary.max_latency}ms
            - Avg: ${results.summary.avg_latency}ms

            ${results.summary.p95_latency <= 20 && results.summary.success_rate >= 99.5 ? '‚úÖ All gates passed!' : '‚ùå Some gates failed!'}`;

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });

  perf-trends:
    name: Performance Trends
    runs-on: ubuntu-latest
    needs: perf-full
    if: github.event_name == 'schedule'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download results
        uses: actions/download-artifact@v4
        with:
          name: full-results
          path: .

      - name: Generate trend analysis
        run: |
          echo "üìà Performance Trend Analysis"
          echo "============================"

          # This would integrate with a time-series database
          # For now, we'll just log the current results
          if [ -f full_results.json ]; then
            echo "Current performance metrics:"
            cat full_results.json | jq '.summary'
            
            # Compare with historical data (if available)
            echo "Trend analysis would be implemented here"
          fi

      - name: Update performance dashboard
        run: |
          echo "Updating performance dashboard..."
          # This would update Grafana or similar dashboard
          # For now, we'll just log the action
          echo "Dashboard update completed"
